{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "waveform",
  "type": "registry:ui",
  "dependencies": [
    "@base-ui-components/react"
  ],
  "files": [
    {
      "path": "registry/default/ui/waveform.tsx",
      "content": "\"use client\";\n\nimport { mergeProps } from \"@base-ui-components/react/merge-props\";\nimport { useRender } from \"@base-ui-components/react/use-render\";\nimport { useCallback, useEffect, useMemo, useRef, useState } from \"react\";\n\nimport { cn } from \"@/lib/utils\";\n\ntype WaveformProps = useRender.ComponentProps<\"div\"> & {\n  data?: number[];\n  barWidth?: number;\n  barGap?: number;\n  barRadius?: number;\n  barColor?: string;\n  fadeEdges?: boolean;\n  fadeWidth?: number;\n  height?: string | number;\n  active?: boolean;\n  onBarClick?: (index: number, value: number) => void;\n};\n\nconst Waveform = ({\n  data = [],\n  barWidth = 4,\n  barGap = 2,\n  barRadius = 2,\n  barColor,\n  fadeEdges = true,\n  fadeWidth = 24,\n  height = 128,\n  onBarClick,\n  className,\n  render,\n  ...props\n}: WaveformProps) => {\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const containerRef = useRef<HTMLDivElement>(null);\n  const heightStyle = typeof height === \"number\" ? `${height}px` : height;\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    const container = containerRef.current;\n    if (!(canvas && container)) {\n      return;\n    }\n\n    const resizeObserver = new ResizeObserver(() => {\n      const rect = container.getBoundingClientRect();\n      const dpr = window.devicePixelRatio || 1;\n\n      canvas.width = rect.width * dpr;\n      canvas.height = rect.height * dpr;\n      canvas.style.width = `${rect.width}px`;\n      canvas.style.height = `${rect.height}px`;\n\n      const ctx = canvas.getContext(\"2d\");\n      if (ctx) {\n        ctx.scale(dpr, dpr);\n        renderWaveform();\n      }\n    });\n\n    const drawBars = (\n      ctx: CanvasRenderingContext2D,\n      rect: DOMRect,\n      computedBarColor: string\n    ) => {\n      const barCount = Math.floor(rect.width / (barWidth + barGap));\n      const centerY = rect.height / 2;\n\n      for (let i = 0; i < barCount; i++) {\n        const dataIndex = Math.floor((i / barCount) * data.length);\n        const value = data[dataIndex] || 0;\n        const barHeight = Math.max(4, value * rect.height * 0.8);\n        const x = i * (barWidth + barGap);\n        const y = centerY - barHeight / 2;\n\n        ctx.fillStyle = computedBarColor;\n        ctx.globalAlpha = 0.3 + value * 0.7;\n\n        if (barRadius > 0) {\n          ctx.beginPath();\n          ctx.roundRect(x, y, barWidth, barHeight, barRadius);\n          ctx.fill();\n        } else {\n          ctx.fillRect(x, y, barWidth, barHeight);\n        }\n      }\n    };\n\n    const applyFadeEdges = (ctx: CanvasRenderingContext2D, rect: DOMRect) => {\n      if (!(fadeEdges && fadeWidth > 0 && rect.width > 0)) {\n        return;\n      }\n\n      const gradient = ctx.createLinearGradient(0, 0, rect.width, 0);\n      const fadePercent = Math.min(0.2, fadeWidth / rect.width);\n\n      gradient.addColorStop(0, \"rgba(255,255,255,1)\");\n      gradient.addColorStop(fadePercent, \"rgba(255,255,255,0)\");\n      gradient.addColorStop(1 - fadePercent, \"rgba(255,255,255,0)\");\n      gradient.addColorStop(1, \"rgba(255,255,255,1)\");\n\n      ctx.globalCompositeOperation = \"destination-out\";\n      ctx.fillStyle = gradient;\n      ctx.fillRect(0, 0, rect.width, rect.height);\n      ctx.globalCompositeOperation = \"source-over\";\n    };\n\n    const renderWaveform = () => {\n      const ctx = canvas.getContext(\"2d\");\n      if (!ctx) {\n        return;\n      }\n\n      const rect = canvas.getBoundingClientRect();\n      ctx.clearRect(0, 0, rect.width, rect.height);\n\n      const computedBarColor =\n        barColor ||\n        getComputedStyle(canvas).getPropertyValue(\"--foreground\") ||\n        \"#000\";\n\n      drawBars(ctx, rect, computedBarColor);\n      applyFadeEdges(ctx, rect);\n      ctx.globalAlpha = 1;\n    };\n\n    resizeObserver.observe(container);\n    renderWaveform();\n\n    return () => resizeObserver.disconnect();\n  }, [data, barWidth, barGap, barRadius, barColor, fadeEdges, fadeWidth]);\n\n  const handleClick = (e: React.MouseEvent<HTMLCanvasElement>) => {\n    if (!onBarClick) {\n      return;\n    }\n\n    const rect = canvasRef.current?.getBoundingClientRect();\n    if (!rect) {\n      return;\n    }\n\n    const x = e.clientX - rect.left;\n    const barIndex = Math.floor(x / (barWidth + barGap));\n    const dataIndex = Math.floor(\n      (barIndex * data.length) / Math.floor(rect.width / (barWidth + barGap))\n    );\n\n    if (dataIndex >= 0 && dataIndex < data.length) {\n      onBarClick?.(dataIndex, data[dataIndex] ?? 0);\n    }\n  };\n\n  const defaultProps = {\n    \"data-slot\": \"waveform\",\n    className: cn(\"relative\", className),\n    style: { height: heightStyle },\n    children: (\n      <canvas\n        className=\"block h-full w-full\"\n        onClick={handleClick}\n        ref={canvasRef}\n      />\n    ),\n  };\n\n  return useRender({\n    defaultTagName: \"div\",\n    render,\n    ref: containerRef,\n    props: mergeProps<\"div\">(defaultProps, props),\n  });\n};\n\ntype ScrollingWaveformProps = Omit<WaveformProps, \"data\" | \"onBarClick\"> & {\n  speed?: number;\n  barCount?: number;\n  data?: number[];\n};\n\nconst ScrollingWaveform = ({\n  speed = 50,\n  barCount = 60,\n  barWidth = 4,\n  barGap = 2,\n  barRadius = 2,\n  barColor,\n  fadeEdges = true,\n  fadeWidth = 24,\n  height = 128,\n  data,\n  className,\n  render,\n  ...props\n}: ScrollingWaveformProps) => {\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const containerRef = useRef<HTMLDivElement>(null);\n  const barsRef = useRef<Array<{ x: number; height: number }>>([]);\n  const animationRef = useRef<number>(0);\n  const lastTimeRef = useRef<number>(0);\n  const seedRef = useRef(Math.random());\n  const dataIndexRef = useRef(0);\n  const heightStyle = typeof height === \"number\" ? `${height}px` : height;\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    const container = containerRef.current;\n    if (!(canvas && container)) {\n      return;\n    }\n\n    const resizeObserver = new ResizeObserver(() => {\n      const rect = container.getBoundingClientRect();\n      const dpr = window.devicePixelRatio || 1;\n\n      canvas.width = rect.width * dpr;\n      canvas.height = rect.height * dpr;\n      canvas.style.width = `${rect.width}px`;\n      canvas.style.height = `${rect.height}px`;\n\n      const ctx = canvas.getContext(\"2d\");\n      if (ctx) {\n        ctx.scale(dpr, dpr);\n      }\n\n      if (barsRef.current.length === 0) {\n        const step = barWidth + barGap;\n        let currentX = rect.width;\n        let index = 0;\n        const seededRandom = (i: number) => {\n          const x = Math.sin(seedRef.current * 10_000 + i) * 10_000;\n          return x - Math.floor(x);\n        };\n        while (currentX > -step) {\n          barsRef.current.push({\n            x: currentX,\n            height: 0.2 + seededRandom(index++) * 0.6,\n          });\n          currentX -= step;\n        }\n      }\n    });\n\n    resizeObserver.observe(container);\n    return () => resizeObserver.disconnect();\n  }, [barWidth, barGap]);\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    if (!canvas) {\n      return;\n    }\n\n    const ctx = canvas.getContext(\"2d\");\n    if (!ctx) {\n      return;\n    }\n\n    const updateBarsPosition = (deltaTime: number) => {\n      for (const bar of barsRef.current) {\n        bar.x -= speed * deltaTime;\n      }\n    };\n\n    const filterOffscreenBars = (step: number) => {\n      barsRef.current = barsRef.current.filter(\n        (bar) => bar.x + barWidth > -step\n      );\n    };\n\n    const calculateNewBarHeight = (): number => {\n      if (data && data.length > 0) {\n        const barHeightValue = data[dataIndexRef.current % data.length] || 0.1;\n        dataIndexRef.current = (dataIndexRef.current + 1) % data.length;\n        return barHeightValue;\n      }\n\n      const time = Date.now() / 1000;\n      const uniqueIndex = barsRef.current.length + time * 0.01;\n      const seededRandom = (index: number) => {\n        const x = Math.sin(seedRef.current * 10_000 + index * 137.5) * 10_000;\n        return x - Math.floor(x);\n      };\n      const wave1 = Math.sin(uniqueIndex * 0.1) * 0.2;\n      const wave2 = Math.cos(uniqueIndex * 0.05) * 0.15;\n      const randomComponent = seededRandom(uniqueIndex) * 0.4;\n      return Math.max(\n        0.1,\n        Math.min(0.9, 0.3 + wave1 + wave2 + randomComponent)\n      );\n    };\n\n    const generateNewBars = (rect: DOMRect, step: number) => {\n      while (\n        barsRef.current.length === 0 ||\n        (barsRef.current.at(-1)?.x ?? 0) < rect.width\n      ) {\n        const lastBar = barsRef.current.at(-1);\n        const nextX = lastBar ? lastBar.x + step : rect.width;\n        const newHeight = calculateNewBarHeight();\n\n        barsRef.current.push({\n          x: nextX,\n          height: newHeight,\n        });\n        if (barsRef.current.length > barCount * 2) {\n          break;\n        }\n      }\n    };\n\n    const drawBars = (\n      context: CanvasRenderingContext2D,\n      bounds: DOMRect,\n      computedBarColor: string\n    ) => {\n      const centerY = bounds.height / 2;\n      for (const bar of barsRef.current) {\n        if (bar.x < bounds.width && bar.x + barWidth > 0) {\n          const barHeight = Math.max(4, bar.height * bounds.height * 0.6);\n          const y = centerY - barHeight / 2;\n\n          context.fillStyle = computedBarColor;\n          context.globalAlpha = 0.3 + bar.height * 0.7;\n\n          if (barRadius > 0) {\n            context.beginPath();\n            context.roundRect(bar.x, y, barWidth, barHeight, barRadius);\n            context.fill();\n          } else {\n            context.fillRect(bar.x, y, barWidth, barHeight);\n          }\n        }\n      }\n    };\n\n    const applyFadeEdges = (\n      context: CanvasRenderingContext2D,\n      bounds: DOMRect\n    ) => {\n      if (!(fadeEdges && fadeWidth > 0)) {\n        return;\n      }\n\n      const gradient = context.createLinearGradient(0, 0, bounds.width, 0);\n      const fadePercent = Math.min(0.2, fadeWidth / bounds.width);\n\n      gradient.addColorStop(0, \"rgba(255,255,255,1)\");\n      gradient.addColorStop(fadePercent, \"rgba(255,255,255,0)\");\n      gradient.addColorStop(1 - fadePercent, \"rgba(255,255,255,0)\");\n      gradient.addColorStop(1, \"rgba(255,255,255,1)\");\n\n      context.globalCompositeOperation = \"destination-out\";\n      context.fillStyle = gradient;\n      context.fillRect(0, 0, bounds.width, bounds.height);\n      context.globalCompositeOperation = \"source-over\";\n    };\n\n    const animate = (currentTime: number) => {\n      const deltaTime = lastTimeRef.current\n        ? (currentTime - lastTimeRef.current) / 1000\n        : 0;\n      lastTimeRef.current = currentTime;\n\n      const rect = canvas.getBoundingClientRect();\n      ctx.clearRect(0, 0, rect.width, rect.height);\n\n      const computedBarColor =\n        barColor ||\n        getComputedStyle(canvas).getPropertyValue(\"--foreground\") ||\n        \"#000\";\n\n      const step = barWidth + barGap;\n      updateBarsPosition(deltaTime);\n      filterOffscreenBars(step);\n      generateNewBars(rect, step);\n      drawBars(ctx, rect, computedBarColor);\n      applyFadeEdges(ctx, rect);\n\n      ctx.globalAlpha = 1;\n      animationRef.current = requestAnimationFrame(animate);\n    };\n\n    animationRef.current = requestAnimationFrame(animate);\n\n    return () => {\n      if (animationRef.current) {\n        cancelAnimationFrame(animationRef.current);\n      }\n    };\n  }, [\n    speed,\n    barCount,\n    barWidth,\n    barGap,\n    barRadius,\n    barColor,\n    fadeEdges,\n    fadeWidth,\n    data,\n  ]);\n\n  const defaultProps = {\n    \"data-slot\": \"scrolling-waveform\",\n    className: cn(\"relative flex items-center\", className),\n    style: { height: heightStyle },\n    children: <canvas className=\"block h-full w-full\" ref={canvasRef} />,\n  };\n\n  return useRender({\n    defaultTagName: \"div\",\n    render,\n    ref: containerRef,\n    props: mergeProps<\"div\">(defaultProps, props),\n  });\n};\n\ntype AudioScrubberProps = WaveformProps & {\n  currentTime?: number;\n  duration?: number;\n  onSeek?: (time: number) => void;\n  showHandle?: boolean;\n};\n\nconst AudioScrubber = ({\n  data = [],\n  currentTime = 0,\n  duration = 100,\n  onSeek,\n  showHandle = true,\n  barWidth = 3,\n  barGap = 1,\n  barRadius = 1,\n  barColor,\n  height = 128,\n  className,\n  render,\n  ...props\n}: AudioScrubberProps) => {\n  const [isDragging, setIsDragging] = useState(false);\n  const [localProgress, setLocalProgress] = useState(0);\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  const waveformData =\n    data.length > 0\n      ? data\n      : Array.from({ length: 100 }, () => 0.2 + Math.random() * 0.6);\n\n  useEffect(() => {\n    if (!isDragging && duration > 0) {\n      setLocalProgress(currentTime / duration);\n    }\n  }, [currentTime, duration, isDragging]);\n\n  const handleScrub = useCallback(\n    (clientX: number) => {\n      const container = containerRef.current;\n      if (!container) {\n        return;\n      }\n\n      const rect = container.getBoundingClientRect();\n      const x = Math.max(0, Math.min(clientX - rect.left, rect.width));\n      const progress = x / rect.width;\n      const newTime = progress * duration;\n\n      setLocalProgress(progress);\n      onSeek?.(newTime);\n    },\n    [duration, onSeek]\n  );\n\n  const handleMouseDown = (e: React.MouseEvent<HTMLDivElement>) => {\n    e.preventDefault();\n    setIsDragging(true);\n    handleScrub(e.clientX);\n  };\n\n  useEffect(() => {\n    if (!isDragging) {\n      return;\n    }\n\n    const handleMouseMove = (e: MouseEvent) => {\n      handleScrub(e.clientX);\n    };\n\n    const handleMouseUp = () => {\n      setIsDragging(false);\n    };\n\n    document.addEventListener(\"mousemove\", handleMouseMove);\n    document.addEventListener(\"mouseup\", handleMouseUp);\n\n    return () => {\n      document.removeEventListener(\"mousemove\", handleMouseMove);\n      document.removeEventListener(\"mouseup\", handleMouseUp);\n    };\n  }, [isDragging, handleScrub]);\n\n  const heightStyle = typeof height === \"number\" ? `${height}px` : height;\n\n  const defaultProps = {\n    \"data-slot\": \"audio-scrubber\",\n    \"aria-label\": \"Audio waveform scrubber\",\n    \"aria-valuemax\": duration,\n    \"aria-valuemin\": 0,\n    \"aria-valuenow\": currentTime,\n    className: cn(\"relative cursor-pointer select-none\", className),\n    onMouseDown: handleMouseDown,\n    role: \"slider\",\n    style: { height: heightStyle },\n    tabIndex: 0,\n    children: (\n      <>\n        <Waveform\n          barColor={barColor}\n          barGap={barGap}\n          barRadius={barRadius}\n          barWidth={barWidth}\n          data={waveformData}\n          fadeEdges={false}\n        />\n\n        <div\n          className=\"pointer-events-none absolute inset-y-0 left-0 bg-primary/20\"\n          style={{ width: `${localProgress * 100}%` }}\n        />\n\n        <div\n          className=\"pointer-events-none absolute top-0 bottom-0 w-0.5 bg-primary\"\n          style={{ left: `${localProgress * 100}%` }}\n        />\n\n        {showHandle && (\n          <div\n            className=\"-translate-x-1/2 -translate-y-1/2 pointer-events-none absolute top-1/2 h-4 w-4 rounded-full border-2 border-background bg-primary shadow-lg transition-transform hover:scale-110\"\n            style={{ left: `${localProgress * 100}%` }}\n          />\n        )}\n      </>\n    ),\n  };\n\n  return useRender({\n    defaultTagName: \"div\",\n    render,\n    ref: containerRef,\n    props: mergeProps<\"div\">(defaultProps, props),\n  });\n};\n\ntype MicrophoneWaveformProps = WaveformProps & {\n  active?: boolean;\n  processing?: boolean;\n  fftSize?: number;\n  smoothingTimeConstant?: number;\n  sensitivity?: number;\n  onError?: (error: Error) => void;\n};\n\nconst MicrophoneWaveform = ({\n  active = false,\n  processing = false,\n  fftSize = 256,\n  smoothingTimeConstant = 0.8,\n  sensitivity = 1,\n  onError,\n  ...props\n}: MicrophoneWaveformProps) => {\n  const [data, setData] = useState<number[]>([]);\n  const analyserRef = useRef<AnalyserNode | null>(null);\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const streamRef = useRef<MediaStream | null>(null);\n  const animationIdRef = useRef<number | null>(null);\n  const processingAnimationRef = useRef<number | null>(null);\n  const lastActiveDataRef = useRef<number[]>([]);\n  const transitionProgressRef = useRef(0);\n\n  useEffect(() => {\n    if (processing && !active) {\n      let time = 0;\n      transitionProgressRef.current = 0;\n\n      const animateProcessing = () => {\n        time += 0.03;\n        transitionProgressRef.current = Math.min(\n          1,\n          transitionProgressRef.current + 0.02\n        );\n\n        const processingData: number[] = [];\n        const barCount = 45;\n\n        for (let i = 0; i < barCount; i++) {\n          const normalizedPosition = (i - barCount / 2) / (barCount / 2);\n          const centerWeight = 1 - Math.abs(normalizedPosition) * 0.4;\n\n          const wave1 = Math.sin(time * 1.5 + i * 0.15) * 0.25;\n          const wave2 = Math.sin(time * 0.8 - i * 0.1) * 0.2;\n          const wave3 = Math.cos(time * 2 + i * 0.05) * 0.15;\n          const combinedWave = wave1 + wave2 + wave3;\n          const processingValue = (0.2 + combinedWave) * centerWeight;\n\n          let finalValue = processingValue;\n          if (\n            lastActiveDataRef.current.length > 0 &&\n            transitionProgressRef.current < 1\n          ) {\n            const lastDataIndex = Math.floor(\n              (i / barCount) * lastActiveDataRef.current.length\n            );\n            const lastValue = lastActiveDataRef.current[lastDataIndex] || 0;\n            finalValue =\n              lastValue * (1 - transitionProgressRef.current) +\n              processingValue * transitionProgressRef.current;\n          }\n\n          processingData.push(Math.max(0.05, Math.min(1, finalValue)));\n        }\n\n        setData(processingData);\n        processingAnimationRef.current =\n          requestAnimationFrame(animateProcessing);\n      };\n\n      animateProcessing();\n\n      return () => {\n        if (processingAnimationRef.current) {\n          cancelAnimationFrame(processingAnimationRef.current);\n        }\n      };\n    }\n    if (!(active || processing)) {\n      if (data.length > 0) {\n        let fadeProgress = 0;\n        const fadeToIdle = () => {\n          fadeProgress += 0.03;\n          if (fadeProgress < 1) {\n            const fadedData = data.map((value) => value * (1 - fadeProgress));\n            setData(fadedData);\n            requestAnimationFrame(fadeToIdle);\n          } else {\n            setData([]);\n          }\n        };\n        fadeToIdle();\n      }\n      return;\n    }\n  }, [processing, active, data.length, data.map]);\n\n  useEffect(() => {\n    const cleanupAudioResources = () => {\n      if (streamRef.current) {\n        for (const track of streamRef.current.getTracks()) {\n          track.stop();\n        }\n      }\n      if (\n        audioContextRef.current &&\n        audioContextRef.current.state !== \"closed\"\n      ) {\n        audioContextRef.current.close();\n      }\n      if (animationIdRef.current) {\n        cancelAnimationFrame(animationIdRef.current);\n      }\n    };\n\n    const normalizeAudioData = (\n      relevantData: Uint8Array,\n      sensitivityValue: number\n    ): number[] => {\n      const halfLength = Math.floor(relevantData.length / 2);\n      const normalizedData: number[] = [];\n\n      for (let i = halfLength - 1; i >= 0; i--) {\n        const dataValue = relevantData[i];\n        if (dataValue !== undefined) {\n          const value = Math.min(1, (dataValue / 255) * sensitivityValue);\n          normalizedData.push(value);\n        }\n      }\n\n      for (let i = 0; i < halfLength; i++) {\n        const dataValue = relevantData[i];\n        if (dataValue !== undefined) {\n          const value = Math.min(1, (dataValue / 255) * sensitivityValue);\n          normalizedData.push(value);\n        }\n      }\n\n      return normalizedData;\n    };\n\n    if (!active) {\n      cleanupAudioResources();\n      return;\n    }\n\n    const setupMicrophone = async () => {\n      try {\n        const stream = await navigator.mediaDevices.getUserMedia({\n          audio: true,\n        });\n        streamRef.current = stream;\n\n        const audioContext = new (\n          window.AudioContext ||\n          (window as unknown as { webkitAudioContext: typeof AudioContext })\n            .webkitAudioContext\n        )();\n        const analyser = audioContext.createAnalyser();\n        analyser.fftSize = fftSize;\n        analyser.smoothingTimeConstant = smoothingTimeConstant;\n\n        const source = audioContext.createMediaStreamSource(stream);\n        source.connect(analyser);\n\n        audioContextRef.current = audioContext;\n        analyserRef.current = analyser;\n\n        const dataArray = new Uint8Array(analyser.frequencyBinCount);\n\n        const updateData = () => {\n          if (!(analyserRef.current && active)) {\n            return;\n          }\n\n          analyserRef.current.getByteFrequencyData(dataArray);\n\n          const startFreq = Math.floor(dataArray.length * 0.05);\n          const endFreq = Math.floor(dataArray.length * 0.4);\n          const relevantData = dataArray.slice(startFreq, endFreq);\n\n          const normalizedData = normalizeAudioData(relevantData, sensitivity);\n\n          setData(normalizedData);\n          lastActiveDataRef.current = normalizedData;\n\n          animationIdRef.current = requestAnimationFrame(updateData);\n        };\n\n        updateData();\n      } catch (error) {\n        onError?.(error as Error);\n      }\n    };\n\n    setupMicrophone();\n\n    return cleanupAudioResources;\n  }, [active, fftSize, smoothingTimeConstant, sensitivity, onError]);\n\n  return <Waveform data={data} {...props} />;\n};\n\ntype StaticWaveformProps = WaveformProps & {\n  bars?: number;\n  seed?: number;\n};\n\nconst StaticWaveform = ({\n  bars = 40,\n  seed = 42,\n  ...props\n}: StaticWaveformProps) => {\n  const data = useMemo(() => {\n    const random = (seedValue: number) => {\n      const x = Math.sin(seedValue) * 10_000;\n      return x - Math.floor(x);\n    };\n\n    return Array.from({ length: bars }, (_, i) => 0.2 + random(seed + i) * 0.6);\n  }, [bars, seed]);\n\n  return <Waveform data={data} {...props} />;\n};\n\ntype LiveMicrophoneWaveformProps = Omit<ScrollingWaveformProps, \"barCount\"> & {\n  active?: boolean;\n  fftSize?: number;\n  smoothingTimeConstant?: number;\n  sensitivity?: number;\n  onError?: (error: Error) => void;\n  historySize?: number;\n  updateRate?: number;\n  savedHistoryRef?: React.RefObject<number[]>;\n  dragOffset?: number;\n  setDragOffset?: (offset: number) => void;\n  enableAudioPlayback?: boolean;\n  playbackRate?: number;\n};\n\nconst calculateDataIndex = (\n  i: number,\n  dataLength: number,\n  offsetInBars: number,\n  isActive: boolean\n): number | null => {\n  let dataIndex: number;\n\n  if (isActive) {\n    dataIndex = dataLength - 1 - i;\n  } else {\n    dataIndex = Math.max(\n      0,\n      Math.min(dataLength - 1, dataLength - 1 - i - Math.floor(offsetInBars))\n    );\n  }\n\n  if (dataIndex >= 0 && dataIndex < dataLength) {\n    return dataIndex;\n  }\n  return null;\n};\n\nconst drawSingleBar = (\n  ctx: CanvasRenderingContext2D,\n  config: {\n    x: number;\n    y: number;\n    barHeight: number;\n    barColor: string;\n    alpha: number;\n    barWidth: number;\n    barRadius: number;\n  }\n) => {\n  ctx.fillStyle = config.barColor;\n  ctx.globalAlpha = config.alpha;\n\n  if (config.barRadius > 0) {\n    ctx.beginPath();\n    ctx.roundRect(\n      config.x,\n      config.y,\n      config.barWidth,\n      config.barHeight,\n      config.barRadius\n    );\n    ctx.fill();\n  } else {\n    ctx.fillRect(config.x, config.y, config.barWidth, config.barHeight);\n  }\n};\n\nconst drawBarsHelper = (\n  ctx: CanvasRenderingContext2D,\n  rect: DOMRect,\n  computedBarColor: string,\n  config: {\n    dataToRender: number[];\n    dragOffset: number;\n    active: boolean;\n    barWidth: number;\n    barGap: number;\n    barRadius: number;\n  }\n) => {\n  const step = config.barWidth + config.barGap;\n  const barCount = Math.floor(rect.width / step);\n  const centerY = rect.height / 2;\n\n  if (config.dataToRender.length === 0) {\n    return;\n  }\n\n  const offsetInBars = Math.floor(config.dragOffset / step);\n\n  for (let i = 0; i < barCount; i++) {\n    const dataIndex = calculateDataIndex(\n      i,\n      config.dataToRender.length,\n      offsetInBars,\n      config.active\n    );\n\n    if (dataIndex === null) {\n      continue;\n    }\n\n    const value = config.dataToRender[dataIndex];\n    if (value !== undefined) {\n      const x = rect.width - (i + 1) * step;\n      const barHeight = Math.max(4, value * rect.height * 0.7);\n      const y = centerY - barHeight / 2;\n\n      drawSingleBar(ctx, {\n        x,\n        y,\n        barHeight,\n        barColor: computedBarColor,\n        alpha: 0.3 + value * 0.7,\n        barWidth: config.barWidth,\n        barRadius: config.barRadius,\n      });\n    }\n  }\n};\n\nconst applyFadeEdgesHelper = (\n  ctx: CanvasRenderingContext2D,\n  rect: DOMRect,\n  fadeEdges: boolean,\n  fadeWidth: number\n) => {\n  if (!(fadeEdges && fadeWidth > 0)) {\n    return;\n  }\n\n  const gradient = ctx.createLinearGradient(0, 0, rect.width, 0);\n  const fadePercent = Math.min(0.2, fadeWidth / rect.width);\n\n  gradient.addColorStop(0, \"rgba(255,255,255,1)\");\n  gradient.addColorStop(fadePercent, \"rgba(255,255,255,0)\");\n  gradient.addColorStop(1 - fadePercent, \"rgba(255,255,255,0)\");\n  gradient.addColorStop(1, \"rgba(255,255,255,1)\");\n\n  ctx.globalCompositeOperation = \"destination-out\";\n  ctx.fillStyle = gradient;\n  ctx.fillRect(0, 0, rect.width, rect.height);\n  ctx.globalCompositeOperation = \"source-over\";\n};\n\nconst updateAudioDataHelper = (\n  currentTime: number,\n  config: {\n    lastUpdateRef: React.RefObject<number>;\n    analyserRef: React.RefObject<AnalyserNode | null>;\n    historyRef: React.RefObject<number[]>;\n    sensitivity: number;\n    updateRate: number;\n    historySize: number;\n  }\n) => {\n  if (currentTime - config.lastUpdateRef.current <= config.updateRate) {\n    return;\n  }\n\n  config.lastUpdateRef.current = currentTime;\n\n  if (!config.analyserRef.current) {\n    return;\n  }\n\n  const dataArray = new Uint8Array(\n    config.analyserRef.current.frequencyBinCount\n  );\n  config.analyserRef.current.getByteFrequencyData(dataArray);\n\n  let sum = 0;\n  for (const value of dataArray) {\n    sum += value;\n  }\n  const average = (sum / dataArray.length / 255) * config.sensitivity;\n\n  config.historyRef.current.push(Math.min(1, Math.max(0.05, average)));\n\n  if (config.historyRef.current.length > config.historySize) {\n    config.historyRef.current.shift();\n  }\n};\n\nconst playScrubSoundHelper = (\n  position: number,\n  direction: number,\n  config: {\n    enableAudioPlayback: boolean;\n    audioBufferRef: React.RefObject<AudioBuffer | null>;\n    audioContextRef: React.RefObject<AudioContext | null>;\n    scrubSourceRef: React.RefObject<AudioBufferSourceNode | null>;\n  }\n) => {\n  if (\n    !(\n      config.enableAudioPlayback &&\n      config.audioBufferRef.current &&\n      config.audioContextRef.current\n    )\n  ) {\n    return;\n  }\n\n  if (config.scrubSourceRef.current) {\n    try {\n      config.scrubSourceRef.current.stop();\n    } catch {\n      //noop\n    }\n  }\n\n  const source = config.audioContextRef.current.createBufferSource();\n  source.buffer = config.audioBufferRef.current;\n\n  const speed = Math.abs(direction);\n  const scrubPlaybackRate =\n    direction > 0\n      ? Math.min(3, 1 + speed * 0.1)\n      : Math.max(-3, -1 - speed * 0.1);\n\n  source.playbackRate.value = scrubPlaybackRate;\n\n  const filter = config.audioContextRef.current.createBiquadFilter();\n  filter.type = \"lowpass\";\n  filter.frequency.value = Math.max(200, 2000 - speed * 100);\n\n  source.connect(filter);\n  filter.connect(config.audioContextRef.current.destination);\n\n  const startTime = Math.max(\n    0,\n    Math.min(position, config.audioBufferRef.current.duration - 0.1)\n  );\n  source.start(0, startTime, 0.1);\n  config.scrubSourceRef.current = source;\n};\n\nconst playFromPositionHelper = (\n  position: number,\n  config: {\n    enableAudioPlayback: boolean;\n    playbackRate: number;\n    audioBufferRef: React.RefObject<AudioBuffer | null>;\n    audioContextRef: React.RefObject<AudioContext | null>;\n    sourceNodeRef: React.RefObject<AudioBufferSourceNode | null>;\n    playbackStartTimeRef: React.RefObject<number>;\n    setPlaybackPosition: (position: number | null) => void;\n  }\n) => {\n  if (\n    !(\n      config.enableAudioPlayback &&\n      config.audioBufferRef.current &&\n      config.audioContextRef.current\n    )\n  ) {\n    return;\n  }\n\n  if (config.sourceNodeRef.current) {\n    try {\n      config.sourceNodeRef.current.stop();\n    } catch {\n      //noop\n    }\n  }\n\n  const source = config.audioContextRef.current.createBufferSource();\n  source.buffer = config.audioBufferRef.current;\n  source.playbackRate.value = config.playbackRate;\n  source.connect(config.audioContextRef.current.destination);\n\n  const startTime = Math.max(\n    0,\n    Math.min(position, config.audioBufferRef.current.duration)\n  );\n  source.start(0, startTime);\n  config.sourceNodeRef.current = source;\n\n  config.playbackStartTimeRef.current =\n    config.audioContextRef.current.currentTime - startTime;\n  config.setPlaybackPosition(startTime);\n\n  source.onended = () => {\n    config.setPlaybackPosition(null);\n  };\n};\n\nconst stopAudioResources = (config: {\n  mediaRecorderRef: React.RefObject<MediaRecorder | null>;\n  streamRef: React.RefObject<MediaStream | null>;\n}) => {\n  if (\n    config.mediaRecorderRef.current &&\n    config.mediaRecorderRef.current.state !== \"inactive\"\n  ) {\n    config.mediaRecorderRef.current.stop();\n  }\n  if (config.streamRef.current) {\n    for (const track of config.streamRef.current.getTracks()) {\n      track.stop();\n    }\n  }\n};\n\nconst createCleanupAudioResources =\n  (config: {\n    mediaRecorderRef: React.RefObject<MediaRecorder | null>;\n    streamRef: React.RefObject<MediaStream | null>;\n    sourceNodeRef: React.RefObject<AudioBufferSourceNode | null>;\n    scrubSourceRef: React.RefObject<AudioBufferSourceNode | null>;\n  }): (() => void) =>\n  (): void => {\n    stopAudioResources(config);\n    if (config.sourceNodeRef.current) {\n      config.sourceNodeRef.current.stop();\n    }\n    if (config.scrubSourceRef.current) {\n      config.scrubSourceRef.current.stop();\n    }\n  };\n\nconst setupMicrophoneAudioHelper = async (config: {\n  streamRef: React.RefObject<MediaStream | null>;\n  audioContextRef: React.RefObject<AudioContext | null>;\n  analyserRef: React.RefObject<AnalyserNode | null>;\n  mediaRecorderRef: React.RefObject<MediaRecorder | null>;\n  audioChunksRef: React.RefObject<Blob[]>;\n  fftSize: number;\n  smoothingTimeConstant: number;\n  enableAudioPlayback: boolean;\n  onError?: (error: Error) => void;\n}) => {\n  try {\n    const stream = await navigator.mediaDevices.getUserMedia({\n      audio: true,\n    });\n    config.streamRef.current = stream;\n\n    const audioContext = new (\n      window.AudioContext ||\n      (window as unknown as { webkitAudioContext: typeof AudioContext })\n        .webkitAudioContext\n    )();\n    const analyser = audioContext.createAnalyser();\n    analyser.fftSize = config.fftSize;\n    analyser.smoothingTimeConstant = config.smoothingTimeConstant;\n\n    const source = audioContext.createMediaStreamSource(stream);\n    source.connect(analyser);\n\n    config.audioContextRef.current = audioContext;\n    config.analyserRef.current = analyser;\n\n    if (config.enableAudioPlayback) {\n      const mediaRecorder = new MediaRecorder(stream);\n      config.mediaRecorderRef.current = mediaRecorder;\n\n      mediaRecorder.ondataavailable = (event) => {\n        if (event.data.size > 0) {\n          config.audioChunksRef.current.push(event.data);\n        }\n      };\n\n      mediaRecorder.start(100);\n    }\n  } catch (error) {\n    config.onError?.(error as Error);\n  }\n};\n\nconst useMicrophoneAudio = (config: {\n  active: boolean;\n  streamRef: React.RefObject<MediaStream | null>;\n  mediaRecorderRef: React.RefObject<MediaRecorder | null>;\n  audioChunksRef: React.RefObject<Blob[]>;\n  audioBufferRef: React.RefObject<AudioBuffer | null>;\n  sourceNodeRef: React.RefObject<AudioBufferSourceNode | null>;\n  scrubSourceRef: React.RefObject<AudioBufferSourceNode | null>;\n  historyRef: React.RefObject<number[]>;\n  audioContextRef: React.RefObject<AudioContext | null>;\n  analyserRef: React.RefObject<AnalyserNode | null>;\n  enableAudioPlayback: boolean;\n  processAudioBlob: (blob: Blob) => Promise<void>;\n  setDragOffset?: (value: number) => void;\n  setPlaybackPosition: (position: number | null) => void;\n  fftSize: number;\n  smoothingTimeConstant: number;\n  onError?: (error: Error) => void;\n}) => {\n  useEffect(() => {\n    if (!config.active) {\n      stopAudioResources({\n        mediaRecorderRef: config.mediaRecorderRef,\n        streamRef: config.streamRef,\n      });\n\n      if (\n        config.enableAudioPlayback &&\n        config.audioChunksRef.current.length > 0\n      ) {\n        const audioBlob = new Blob(config.audioChunksRef.current, {\n          type: \"audio/webm\",\n        });\n        config.processAudioBlob(audioBlob);\n      }\n\n      return createCleanupAudioResources({\n        mediaRecorderRef: config.mediaRecorderRef,\n        streamRef: config.streamRef,\n        sourceNodeRef: config.sourceNodeRef,\n        scrubSourceRef: config.scrubSourceRef,\n      });\n    }\n\n    config.setDragOffset?.(0);\n    config.historyRef.current = [];\n    config.audioChunksRef.current = [];\n    config.audioBufferRef.current = null;\n    config.setPlaybackPosition(null);\n\n    setupMicrophoneAudioHelper({\n      streamRef: config.streamRef,\n      audioContextRef: config.audioContextRef,\n      analyserRef: config.analyserRef,\n      mediaRecorderRef: config.mediaRecorderRef,\n      audioChunksRef: config.audioChunksRef,\n      fftSize: config.fftSize,\n      smoothingTimeConstant: config.smoothingTimeConstant,\n      enableAudioPlayback: config.enableAudioPlayback,\n      onError: config.onError,\n    });\n\n    return createCleanupAudioResources({\n      mediaRecorderRef: config.mediaRecorderRef,\n      streamRef: config.streamRef,\n      sourceNodeRef: config.sourceNodeRef,\n      scrubSourceRef: config.scrubSourceRef,\n    });\n  }, [\n    config.active,\n    config.setDragOffset,\n    config.enableAudioPlayback,\n    config.historyRef,\n    config.processAudioBlob,\n    config.fftSize,\n    config.smoothingTimeConstant,\n    config.onError,\n    config.analyserRef,\n    config.audioChunksRef,\n    config.audioBufferRef,\n    config.audioContextRef,\n    config.mediaRecorderRef,\n    config.scrubSourceRef,\n    config.setPlaybackPosition,\n    config.sourceNodeRef,\n    config.streamRef,\n  ]);\n};\n\nconst usePlaybackVisual = (config: {\n  playbackPosition: number | null;\n  playbackRate: number;\n  audioContextRef: React.RefObject<AudioContext | null>;\n  sourceNodeRef: React.RefObject<AudioBufferSourceNode | null>;\n  audioBufferRef: React.RefObject<AudioBuffer | null>;\n  playbackStartTimeRef: React.RefObject<number>;\n  historyRef: React.RefObject<number[]>;\n  containerRef: React.RefObject<HTMLDivElement | null>;\n  barWidth: number;\n  barGap: number;\n  setDragOffset?: (value: number) => void;\n  setPlaybackPosition: (position: number | null) => void;\n}) => {\n  useEffect(() => {\n    if (config.playbackPosition === null || !config.audioBufferRef.current) {\n      return;\n    }\n\n    let animationId: number;\n    const updatePlaybackVisual = () => {\n      if (\n        config.audioContextRef.current &&\n        config.sourceNodeRef.current &&\n        config.audioBufferRef.current\n      ) {\n        const elapsed =\n          config.audioContextRef.current.currentTime -\n          config.playbackStartTimeRef.current;\n        const currentPos =\n          (config.playbackPosition ?? 0) + elapsed * config.playbackRate;\n\n        if (currentPos < config.audioBufferRef.current.duration) {\n          const progressRatio =\n            currentPos / config.audioBufferRef.current.duration;\n          const currentBarIndex = Math.floor(\n            progressRatio * config.historyRef.current.length\n          );\n          const step = config.barWidth + config.barGap;\n\n          const containerWidth =\n            config.containerRef.current?.getBoundingClientRect().width || 0;\n          const viewBars = Math.floor(containerWidth / step);\n          const targetOffset =\n            -(currentBarIndex - (config.historyRef.current.length - viewBars)) *\n            step;\n          const clampedOffset = Math.max(\n            -(config.historyRef.current.length - viewBars) * step,\n            Math.min(0, targetOffset)\n          );\n\n          config.setDragOffset?.(clampedOffset);\n          animationId = requestAnimationFrame(updatePlaybackVisual);\n        } else {\n          config.setPlaybackPosition(null);\n          const step = config.barWidth + config.barGap;\n          const containerWidth =\n            config.containerRef.current?.getBoundingClientRect().width || 0;\n          const viewBars = Math.floor(containerWidth / step);\n          config.setDragOffset?.(\n            -(config.historyRef.current.length - viewBars) * step\n          );\n        }\n      }\n    };\n\n    animationId = requestAnimationFrame(updatePlaybackVisual);\n\n    return () => {\n      if (animationId) {\n        cancelAnimationFrame(animationId);\n      }\n    };\n  }, [\n    config.playbackPosition,\n    config.playbackRate,\n    config.barWidth,\n    config.barGap,\n    config.setDragOffset,\n    config.setPlaybackPosition,\n    config.historyRef,\n    config.audioBufferRef,\n    config.audioContextRef,\n    config.sourceNodeRef,\n    config.playbackStartTimeRef,\n    config.containerRef,\n  ]);\n};\n\nconst useDragHandler = (config: {\n  isDragging: boolean;\n  setIsDragging: (value: boolean) => void;\n  dragStartXRef: React.RefObject<number>;\n  dragStartOffsetRef: React.RefObject<number>;\n  canvasRef: React.RefObject<HTMLCanvasElement | null>;\n  historyRef: React.RefObject<number[]>;\n  audioBufferRef: React.RefObject<AudioBuffer | null>;\n  scrubSourceRef: React.RefObject<AudioBufferSourceNode | null>;\n  dragOffset: number;\n  barWidth: number;\n  barGap: number;\n  enableAudioPlayback: boolean;\n  setDragOffset?: (value: number) => void;\n  playScrubSound: (position: number, direction: number) => void;\n  playFromPosition: (position: number) => void;\n}) => {\n  useEffect(() => {\n    if (!config.isDragging) {\n      return;\n    }\n\n    let lastScrubTime = 0;\n    let lastMouseX = config.dragStartXRef.current;\n    const handleMouseMove = (e: MouseEvent) => {\n      const deltaX = e.clientX - config.dragStartXRef.current;\n      const newOffset = config.dragStartOffsetRef.current - deltaX * 0.5;\n\n      const step = config.barWidth + config.barGap;\n      const maxBars = config.historyRef.current.length;\n      const viewWidth =\n        config.canvasRef.current?.getBoundingClientRect().width || 0;\n      const viewBars = Math.floor(viewWidth / step);\n\n      const maxOffset = Math.max(0, (maxBars - viewBars) * step);\n      const minOffset = 0;\n      const clampedOffset = Math.max(minOffset, Math.min(maxOffset, newOffset));\n\n      config.setDragOffset?.(clampedOffset);\n\n      const now = Date.now();\n      if (\n        config.enableAudioPlayback &&\n        config.audioBufferRef.current &&\n        now - lastScrubTime > 50\n      ) {\n        lastScrubTime = now;\n        const offsetBars = Math.floor(clampedOffset / step);\n        const rightmostBarIndex = Math.max(\n          0,\n          Math.min(maxBars - 1, maxBars - 1 - offsetBars)\n        );\n        const audioPosition =\n          (rightmostBarIndex / maxBars) *\n          config.audioBufferRef.current.duration;\n        const direction = e.clientX - lastMouseX;\n        lastMouseX = e.clientX;\n        config.playScrubSound(\n          Math.max(\n            0,\n            Math.min(\n              config.audioBufferRef.current.duration - 0.1,\n              audioPosition\n            )\n          ),\n          direction\n        );\n      }\n    };\n\n    const handleMouseUp = () => {\n      config.setIsDragging(false);\n\n      if (config.enableAudioPlayback && config.audioBufferRef.current) {\n        const step = config.barWidth + config.barGap;\n        const maxBars = config.historyRef.current.length;\n        const offsetBars = Math.floor(config.dragOffset / step);\n        const rightmostBarIndex = Math.max(\n          0,\n          Math.min(maxBars - 1, maxBars - 1 - offsetBars)\n        );\n        const audioPosition =\n          (rightmostBarIndex / maxBars) *\n          config.audioBufferRef.current.duration;\n        config.playFromPosition(\n          Math.max(\n            0,\n            Math.min(\n              config.audioBufferRef.current.duration - 0.1,\n              audioPosition\n            )\n          )\n        );\n      }\n\n      if (config.scrubSourceRef.current) {\n        try {\n          config.scrubSourceRef.current.stop();\n        } catch {\n          //noop\n        }\n      }\n    };\n\n    document.addEventListener(\"mousemove\", handleMouseMove);\n    document.addEventListener(\"mouseup\", handleMouseUp);\n\n    return () => {\n      document.removeEventListener(\"mousemove\", handleMouseMove);\n      document.removeEventListener(\"mouseup\", handleMouseUp);\n    };\n  }, [\n    config.isDragging,\n    config.setIsDragging,\n    config.dragStartXRef,\n    config.dragStartOffsetRef,\n    config.canvasRef,\n    config.historyRef,\n    config.audioBufferRef,\n    config.scrubSourceRef,\n    config.dragOffset,\n    config.barWidth,\n    config.barGap,\n    config.enableAudioPlayback,\n    config.setDragOffset,\n    config.playScrubSound,\n    config.playFromPosition,\n  ]);\n};\n\nconst createCanvasAnimation = (\n  canvas: HTMLCanvasElement,\n  config: {\n    active: boolean;\n    historyRef: React.RefObject<number[]>;\n    playbackPosition: number | null;\n    dragOffset: number;\n    barWidth: number;\n    barGap: number;\n    barRadius: number;\n    barColor?: string;\n    fadeEdges: boolean;\n    fadeWidth: number;\n    lastUpdateRef: React.RefObject<number>;\n    analyserRef: React.RefObject<AnalyserNode | null>;\n    sensitivity: number;\n    updateRate: number;\n    historySize: number;\n    animationRef: React.RefObject<number>;\n  }\n): (() => void) => {\n  const ctx = canvas.getContext(\"2d\");\n  if (!ctx) {\n    return () => {\n      // No cleanup needed if context is not available\n    };\n  }\n\n  const animate = (currentTime: number) => {\n    if (config.active) {\n      updateAudioDataHelper(currentTime, {\n        lastUpdateRef: config.lastUpdateRef,\n        analyserRef: config.analyserRef,\n        historyRef: config.historyRef,\n        sensitivity: config.sensitivity,\n        updateRate: config.updateRate,\n        historySize: config.historySize,\n      });\n    }\n\n    const rect = canvas.getBoundingClientRect();\n    ctx.clearRect(0, 0, rect.width, rect.height);\n\n    const computedBarColor =\n      config.barColor ||\n      getComputedStyle(canvas).getPropertyValue(\"--foreground\") ||\n      \"#000\";\n\n    drawBarsHelper(ctx, rect, computedBarColor, {\n      dataToRender: config.historyRef.current,\n      dragOffset: config.dragOffset,\n      active: config.active,\n      barWidth: config.barWidth,\n      barGap: config.barGap,\n      barRadius: config.barRadius,\n    });\n    applyFadeEdgesHelper(ctx, rect, config.fadeEdges, config.fadeWidth);\n    ctx.globalAlpha = 1;\n\n    config.animationRef.current = requestAnimationFrame(animate);\n  };\n\n  if (\n    config.active ||\n    config.historyRef.current.length > 0 ||\n    config.playbackPosition !== null\n  ) {\n    config.animationRef.current = requestAnimationFrame(animate);\n  }\n\n  return () => {\n    if (config.animationRef.current) {\n      cancelAnimationFrame(config.animationRef.current);\n    }\n  };\n};\n\nconst LiveMicrophoneWaveform = ({\n  active = false,\n  fftSize = 256,\n  smoothingTimeConstant = 0.8,\n  sensitivity = 1,\n  onError,\n  historySize = 150,\n  updateRate = 50,\n  barWidth = 3,\n  barGap = 1,\n  barRadius = 1,\n  barColor,\n  fadeEdges = true,\n  fadeWidth = 24,\n  height = 128,\n  className,\n  savedHistoryRef,\n  dragOffset: externalDragOffset,\n  setDragOffset: externalSetDragOffset,\n  enableAudioPlayback = true,\n  playbackRate = 1,\n  render,\n  ...props\n}: LiveMicrophoneWaveformProps) => {\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const containerRef = useRef<HTMLDivElement>(null);\n  const internalHistoryRef = useRef<number[]>([]);\n  const historyRef = savedHistoryRef || internalHistoryRef;\n  const analyserRef = useRef<AnalyserNode | null>(null);\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const streamRef = useRef<MediaStream | null>(null);\n  const animationRef = useRef<number>(0);\n  const lastUpdateRef = useRef<number>(0);\n  const [internalDragOffset, setInternalDragOffset] = useState(0);\n  const [isDragging, setIsDragging] = useState(false);\n  const [playbackPosition, setPlaybackPosition] = useState<number | null>(null);\n  const dragStartXRef = useRef<number>(0);\n  const dragStartOffsetRef = useRef<number>(0);\n  const playbackStartTimeRef = useRef<number>(0);\n\n  // Audio recording and playback refs\n  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n  const audioChunksRef = useRef<Blob[]>([]);\n  const audioBufferRef = useRef<AudioBuffer | null>(null);\n  const sourceNodeRef = useRef<AudioBufferSourceNode | null>(null);\n  const scrubSourceRef = useRef<AudioBufferSourceNode | null>(null);\n\n  // Use external drag state if provided, otherwise use internal\n  const dragOffset = externalDragOffset ?? internalDragOffset;\n  const setDragOffset = externalSetDragOffset ?? setInternalDragOffset;\n\n  const heightStyle = typeof height === \"number\" ? `${height}px` : height;\n\n  const isInteractive = !active && historyRef.current.length > 0;\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    const container = containerRef.current;\n    if (!(canvas && container)) {\n      return;\n    }\n\n    const resizeObserver = new ResizeObserver(() => {\n      const rect = container.getBoundingClientRect();\n      const dpr = window.devicePixelRatio || 1;\n\n      canvas.width = rect.width * dpr;\n      canvas.height = rect.height * dpr;\n      canvas.style.width = `${rect.width}px`;\n      canvas.style.height = `${rect.height}px`;\n\n      const ctx = canvas.getContext(\"2d\");\n      if (ctx) {\n        ctx.scale(dpr, dpr);\n      }\n    });\n\n    resizeObserver.observe(container);\n    return () => resizeObserver.disconnect();\n  }, []);\n\n  const processAudioBlob = useCallback(async (blob: Blob) => {\n    try {\n      const arrayBuffer = await blob.arrayBuffer();\n      if (audioContextRef.current) {\n        const audioBuffer =\n          await audioContextRef.current.decodeAudioData(arrayBuffer);\n        audioBufferRef.current = audioBuffer;\n      }\n    } catch (error) {\n      console.error(\"Error processing audio:\", error);\n    }\n  }, []);\n\n  useMicrophoneAudio({\n    active,\n    streamRef,\n    mediaRecorderRef,\n    audioChunksRef,\n    audioBufferRef,\n    sourceNodeRef,\n    scrubSourceRef,\n    historyRef,\n    audioContextRef,\n    analyserRef,\n    enableAudioPlayback,\n    processAudioBlob,\n    setDragOffset,\n    setPlaybackPosition,\n    fftSize,\n    smoothingTimeConstant,\n    onError,\n  });\n\n  const playScrubSound = useCallback(\n    (position: number, direction: number) => {\n      playScrubSoundHelper(position, direction, {\n        enableAudioPlayback,\n        audioBufferRef,\n        audioContextRef,\n        scrubSourceRef,\n      });\n    },\n    [enableAudioPlayback]\n  );\n\n  const playFromPosition = useCallback(\n    (position: number) => {\n      playFromPositionHelper(position, {\n        enableAudioPlayback,\n        playbackRate,\n        audioBufferRef,\n        audioContextRef,\n        sourceNodeRef,\n        playbackStartTimeRef,\n        setPlaybackPosition,\n      });\n    },\n    [enableAudioPlayback, playbackRate]\n  );\n\n  usePlaybackVisual({\n    playbackPosition,\n    playbackRate,\n    audioContextRef,\n    sourceNodeRef,\n    audioBufferRef,\n    playbackStartTimeRef,\n    historyRef,\n    containerRef,\n    barWidth,\n    barGap,\n    setDragOffset,\n    setPlaybackPosition,\n  });\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    if (!canvas) {\n      return;\n    }\n    if (\n      !active &&\n      historyRef.current.length === 0 &&\n      playbackPosition === null\n    ) {\n      return;\n    }\n\n    return createCanvasAnimation(canvas, {\n      active,\n      historyRef,\n      playbackPosition,\n      dragOffset,\n      barWidth,\n      barGap,\n      barRadius,\n      barColor,\n      fadeEdges,\n      fadeWidth,\n      lastUpdateRef,\n      analyserRef,\n      sensitivity,\n      updateRate,\n      historySize,\n      animationRef,\n    });\n  }, [\n    active,\n    sensitivity,\n    updateRate,\n    historySize,\n    barWidth,\n    barGap,\n    barRadius,\n    barColor,\n    fadeEdges,\n    fadeWidth,\n    dragOffset,\n    playbackPosition,\n    historyRef,\n  ]);\n\n  const handleMouseDown = (e: React.MouseEvent<HTMLDivElement>) => {\n    if (active || historyRef.current.length === 0) {\n      return;\n    }\n\n    e.preventDefault();\n    setIsDragging(true);\n    dragStartXRef.current = e.clientX;\n    dragStartOffsetRef.current = dragOffset;\n  };\n\n  useDragHandler({\n    isDragging,\n    setIsDragging,\n    dragStartXRef,\n    dragStartOffsetRef,\n    canvasRef,\n    historyRef,\n    audioBufferRef,\n    scrubSourceRef,\n    dragOffset,\n    barWidth,\n    barGap,\n    enableAudioPlayback,\n    setDragOffset,\n    playScrubSound,\n    playFromPosition,\n  });\n\n  const defaultProps = {\n    \"data-slot\": \"live-microphone-waveform\",\n    \"aria-label\": isInteractive ? \"Drag to scrub through recording\" : undefined,\n    \"aria-valuemax\": isInteractive ? historyRef.current.length : undefined,\n    \"aria-valuemin\": isInteractive ? 0 : undefined,\n    \"aria-valuenow\": isInteractive ? Math.abs(dragOffset) : undefined,\n    className: cn(\n      \"relative flex items-center\",\n      isInteractive && \"cursor-pointer\",\n      className\n    ),\n    onMouseDown: handleMouseDown,\n    role: isInteractive ? \"slider\" : undefined,\n    style: { height: heightStyle },\n    tabIndex: isInteractive ? 0 : undefined,\n    children: <canvas className=\"block h-full w-full\" ref={canvasRef} />,\n  };\n\n  return useRender({\n    defaultTagName: \"div\",\n    render,\n    ref: containerRef,\n    props: mergeProps<\"div\">(defaultProps, props),\n  });\n};\n\ntype RecordingWaveformProps = Omit<WaveformProps, \"data\" | \"onBarClick\"> & {\n  recording?: boolean;\n  fftSize?: number;\n  smoothingTimeConstant?: number;\n  sensitivity?: number;\n  onError?: (error: Error) => void;\n  onRecordingComplete?: (data: number[]) => void;\n  updateRate?: number;\n  showHandle?: boolean;\n};\n\nconst RecordingWaveform = ({\n  recording = false,\n  fftSize = 256,\n  smoothingTimeConstant = 0.8,\n  sensitivity = 1,\n  onError,\n  onRecordingComplete,\n  updateRate = 50,\n  showHandle = true,\n  barWidth = 3,\n  barGap = 1,\n  barRadius = 1,\n  barColor,\n  height = 128,\n  className,\n  render,\n  ...props\n}: RecordingWaveformProps) => {\n  const [recordedData, setRecordedData] = useState<number[]>([]);\n  const [viewPosition, setViewPosition] = useState(1);\n  const [isDragging, setIsDragging] = useState(false);\n  const [isRecordingComplete, setIsRecordingComplete] = useState(false);\n\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const containerRef = useRef<HTMLDivElement>(null);\n  const recordingDataRef = useRef<number[]>([]);\n  const analyserRef = useRef<AnalyserNode | null>(null);\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const streamRef = useRef<MediaStream | null>(null);\n  const animationRef = useRef<number>(0);\n  const lastUpdateRef = useRef<number>(0);\n\n  const heightStyle = typeof height === \"number\" ? `${height}px` : height;\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    const container = containerRef.current;\n    if (!(canvas && container)) {\n      return;\n    }\n\n    const resizeObserver = new ResizeObserver(() => {\n      const rect = container.getBoundingClientRect();\n      const dpr = window.devicePixelRatio || 1;\n\n      canvas.width = rect.width * dpr;\n      canvas.height = rect.height * dpr;\n      canvas.style.width = `${rect.width}px`;\n      canvas.style.height = `${rect.height}px`;\n\n      const ctx = canvas.getContext(\"2d\");\n      if (ctx) {\n        ctx.scale(dpr, dpr);\n      }\n    });\n\n    resizeObserver.observe(container);\n    return () => resizeObserver.disconnect();\n  }, []);\n\n  useEffect(() => {\n    const cleanupAudioResources = () => {\n      if (streamRef.current) {\n        for (const track of streamRef.current.getTracks()) {\n          track.stop();\n        }\n      }\n      if (\n        audioContextRef.current &&\n        audioContextRef.current.state !== \"closed\"\n      ) {\n        audioContextRef.current.close();\n      }\n    };\n\n    const finalizeRecording = () => {\n      if (recordingDataRef.current.length > 0) {\n        setRecordedData([...recordingDataRef.current]);\n        setIsRecordingComplete(true);\n        onRecordingComplete?.(recordingDataRef.current);\n      }\n    };\n\n    const resetRecordingState = () => {\n      setIsRecordingComplete(false);\n      recordingDataRef.current = [];\n      setRecordedData([]);\n      setViewPosition(1);\n    };\n\n    const setupMicrophone = async () => {\n      try {\n        const stream = await navigator.mediaDevices.getUserMedia({\n          audio: true,\n        });\n        streamRef.current = stream;\n\n        const audioContext = new (\n          window.AudioContext ||\n          (window as unknown as { webkitAudioContext: typeof AudioContext })\n            .webkitAudioContext\n        )();\n        const analyser = audioContext.createAnalyser();\n        analyser.fftSize = fftSize;\n        analyser.smoothingTimeConstant = smoothingTimeConstant;\n\n        const source = audioContext.createMediaStreamSource(stream);\n        source.connect(analyser);\n\n        audioContextRef.current = audioContext;\n        analyserRef.current = analyser;\n      } catch (error) {\n        onError?.(error as Error);\n      }\n    };\n\n    if (!recording) {\n      cleanupAudioResources();\n      finalizeRecording();\n      return;\n    }\n\n    resetRecordingState();\n    setupMicrophone();\n\n    return cleanupAudioResources;\n  }, [recording, fftSize, smoothingTimeConstant, onError, onRecordingComplete]);\n\n  useEffect(() => {\n    const canvas = canvasRef.current;\n    if (!canvas) {\n      return;\n    }\n\n    const ctx = canvas.getContext(\"2d\");\n    if (!ctx) {\n      return;\n    }\n\n    const updateRecordingData = (currentTime: number) => {\n      if (recording && currentTime - lastUpdateRef.current > updateRate) {\n        lastUpdateRef.current = currentTime;\n\n        if (analyserRef.current) {\n          const dataArray = new Uint8Array(\n            analyserRef.current.frequencyBinCount\n          );\n          analyserRef.current.getByteFrequencyData(dataArray);\n\n          let sum = 0;\n          for (const value of dataArray) {\n            sum += value;\n          }\n          const average = (sum / dataArray.length / 255) * sensitivity;\n\n          recordingDataRef.current.push(Math.min(1, Math.max(0.05, average)));\n        }\n      }\n    };\n\n    const calculateStartIndex = (\n      dataLength: number,\n      barsVisible: number\n    ): number => {\n      if (!recording && isRecordingComplete) {\n        if (dataLength > barsVisible) {\n          return Math.floor((dataLength - barsVisible) * viewPosition);\n        }\n        return 0;\n      }\n      if (recording) {\n        return Math.max(0, dataLength - barsVisible);\n      }\n      return 0;\n    };\n\n    const drawBars = (\n      context: CanvasRenderingContext2D,\n      config: {\n        rect: DOMRect;\n        dataToRender: number[];\n        startIndex: number;\n        computedBarColor: string;\n      }\n    ) => {\n      const step = barWidth + barGap;\n      const barsVisible = Math.floor(config.rect.width / step);\n      const centerY = config.rect.height / 2;\n\n      for (\n        let i = 0;\n        i < barsVisible && config.startIndex + i < config.dataToRender.length;\n        i++\n      ) {\n        const value = config.dataToRender[config.startIndex + i] || 0.1;\n        const x = i * step;\n        const barHeight = Math.max(4, value * config.rect.height * 0.7);\n        const y = centerY - barHeight / 2;\n\n        context.fillStyle = config.computedBarColor;\n        context.globalAlpha = 0.3 + value * 0.7;\n\n        if (barRadius > 0) {\n          context.beginPath();\n          context.roundRect(x, y, barWidth, barHeight, barRadius);\n          context.fill();\n        } else {\n          context.fillRect(x, y, barWidth, barHeight);\n        }\n      }\n    };\n\n    const drawHandle = (\n      context: CanvasRenderingContext2D,\n      rect: DOMRect,\n      computedBarColor: string\n    ) => {\n      if (!recording && isRecordingComplete && showHandle) {\n        const indicatorX = rect.width * viewPosition;\n        const centerY = rect.height / 2;\n\n        context.strokeStyle = computedBarColor;\n        context.globalAlpha = 0.5;\n        context.lineWidth = 2;\n        context.beginPath();\n        context.moveTo(indicatorX, 0);\n        context.lineTo(indicatorX, rect.height);\n        context.stroke();\n        context.fillStyle = computedBarColor;\n        context.globalAlpha = 1;\n        context.beginPath();\n        context.arc(indicatorX, centerY, 6, 0, Math.PI * 2);\n        context.fill();\n      }\n    };\n\n    const animate = (currentTime: number) => {\n      updateRecordingData(currentTime);\n\n      const rect = canvas.getBoundingClientRect();\n      ctx.clearRect(0, 0, rect.width, rect.height);\n\n      const computedBarColor =\n        barColor ||\n        getComputedStyle(canvas).getPropertyValue(\"--foreground\") ||\n        \"#000\";\n\n      const dataToRender = recording ? recordingDataRef.current : recordedData;\n\n      if (dataToRender.length > 0) {\n        const step = barWidth + barGap;\n        const barsVisible = Math.floor(rect.width / step);\n        const startIndex = calculateStartIndex(\n          dataToRender.length,\n          barsVisible\n        );\n\n        drawBars(ctx, {\n          rect,\n          dataToRender,\n          startIndex,\n          computedBarColor,\n        });\n        drawHandle(ctx, rect, computedBarColor);\n      }\n\n      ctx.globalAlpha = 1;\n      animationRef.current = requestAnimationFrame(animate);\n    };\n\n    animationRef.current = requestAnimationFrame(animate);\n\n    return () => {\n      if (animationRef.current) {\n        cancelAnimationFrame(animationRef.current);\n      }\n    };\n  }, [\n    recording,\n    recordedData,\n    viewPosition,\n    isRecordingComplete,\n    sensitivity,\n    updateRate,\n    showHandle,\n    barWidth,\n    barGap,\n    barRadius,\n    barColor,\n  ]);\n\n  const handleScrub = useCallback(\n    (clientX: number) => {\n      const container = containerRef.current;\n      if (!container || recording || !isRecordingComplete) {\n        return;\n      }\n\n      const rect = container.getBoundingClientRect();\n      const x = Math.max(0, Math.min(clientX - rect.left, rect.width));\n      const position = x / rect.width;\n\n      setViewPosition(position);\n    },\n    [recording, isRecordingComplete]\n  );\n\n  const handleMouseDown = (e: React.MouseEvent<HTMLDivElement>) => {\n    if (recording || !isRecordingComplete) {\n      return;\n    }\n\n    e.preventDefault();\n    setIsDragging(true);\n    handleScrub(e.clientX);\n  };\n\n  useEffect(() => {\n    if (!isDragging) {\n      return;\n    }\n\n    const handleMouseMove = (e: MouseEvent) => {\n      handleScrub(e.clientX);\n    };\n\n    const handleMouseUp = () => {\n      setIsDragging(false);\n    };\n\n    document.addEventListener(\"mousemove\", handleMouseMove);\n    document.addEventListener(\"mouseup\", handleMouseUp);\n\n    return () => {\n      document.removeEventListener(\"mousemove\", handleMouseMove);\n      document.removeEventListener(\"mouseup\", handleMouseUp);\n    };\n  }, [isDragging, handleScrub]);\n\n  const isInteractive = isRecordingComplete && !recording;\n\n  const defaultProps = {\n    \"data-slot\": \"recording-waveform\",\n    \"aria-label\": isInteractive ? \"Drag to scrub through recording\" : undefined,\n    \"aria-valuemax\": isInteractive ? 100 : undefined,\n    \"aria-valuemin\": isInteractive ? 0 : undefined,\n    \"aria-valuenow\": isInteractive ? viewPosition * 100 : undefined,\n    className: cn(\n      \"relative flex items-center\",\n      isInteractive && \"cursor-pointer\",\n      className\n    ),\n    onMouseDown: handleMouseDown,\n    role: isInteractive ? \"slider\" : undefined,\n    style: { height: heightStyle },\n    tabIndex: isInteractive ? 0 : undefined,\n    children: <canvas className=\"block h-full w-full\" ref={canvasRef} />,\n  };\n\n  return useRender({\n    defaultTagName: \"div\",\n    render,\n    ref: containerRef,\n    props: mergeProps<\"div\">(defaultProps, props),\n  });\n};\n\nexport {\n  Waveform,\n  ScrollingWaveform,\n  AudioScrubber,\n  MicrophoneWaveform,\n  StaticWaveform,\n  LiveMicrophoneWaveform,\n  RecordingWaveform,\n};\n",
      "type": "registry:ui"
    }
  ]
}